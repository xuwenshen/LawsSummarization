{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 998, 1, 450]\n",
      "[None, 996, 1, 450]\n",
      "[None, 994, 1, 450]\n",
      "build model \n",
      "embedding:0\n",
      "conv_source_w_1:0\n",
      "conv_source_w_2:0\n",
      "conv_source_w_3:0\n",
      "conv_source_b_1:0\n",
      "conv_source_b_2:0\n",
      "conv_source_b_3:0\n",
      "conv_source_scale_1:0\n",
      "conv_source_scale_2:0\n",
      "conv_source_scale_3:0\n",
      "conv_source_offset_1:0\n",
      "conv_source_offset_2:0\n",
      "conv_source_offset_3:0\n",
      "generation_w:0\n",
      "generation_b:0\n",
      "dynamic_rnn_encoder/BasicLSTMCell/Linear/Matrix:0\n",
      "dynamic_rnn_encoder/BasicLSTMCell/Linear/Bias:0\n",
      "rnn_decoder/AttnW_0:0\n",
      "rnn_decoder/AttnV_0:0\n",
      "rnn_decoder/Linear/Matrix:0\n",
      "rnn_decoder/Linear/Bias:0\n",
      "rnn_decoder/BasicLSTMCell/Linear/Matrix:0\n",
      "rnn_decoder/BasicLSTMCell/Linear/Bias:0\n",
      "rnn_decoder/Attention_0/Linear/Matrix:0\n",
      "rnn_decoder/Attention_0/Linear/Bias:0\n",
      "rnn_decoder/AttnOutputProjection/Linear/Matrix:0\n",
      "rnn_decoder/AttnOutputProjection/Linear/Bias:0\n",
      "/home/xuwenshen/2017_3_13/attention_cnn_lstm-lstm/model_v1/sample_rate-0.5-train_accu-0.000-train_cost-0.695-train_bleu-0.000-valid_accu-0.313-valid_cost-4.354-valid_bleu-0.652-model.ckpt-67\n",
      "Loss = 4.07891 Accuracy = 0.34446 BLEU = 0.67229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuwenshen/py3/lib/python3.4/site-packages/nltk/translate/bleu_score.py:472: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell, seq2seq\n",
    "from tqdm import *\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from Utils import Utils\n",
    "import attention_cnn_lstm_lstm as model\n",
    "\n",
    "\n",
    "    \n",
    "def test(test_utils, source_len, simplified_len, oseq_len, embedding_size, batch_size, decoder_hidden,\n",
    "          encoder_hidden, source_nfilters, source_width, lstm_layer, model_dir):\n",
    "    \n",
    "    words_size = test_utils.get_words_size()\n",
    "    \n",
    "    outputs = model.build_model(words_size=words_size, \n",
    "                                embedding_size=embedding_size,\n",
    "                                source_len=source_len,\n",
    "                                simplified_len=simplified_len,\n",
    "                                oseq_len=oseq_len, \n",
    "                                decoder_hidden=decoder_hidden,\n",
    "                                encoder_hidden=encoder_hidden,\n",
    "                                source_nfilters=source_nfilters,\n",
    "                                source_width=source_width,\n",
    "                                lstm_layer=lstm_layer, \n",
    "                                batch_size=batch_size, \n",
    "                                is_train=False)\n",
    "\n",
    "    cost=outputs['cost']\n",
    "    words_prediction=outputs['words_prediction']\n",
    "    source=outputs['source']\n",
    "    defendant=outputs['defendant']\n",
    "    defendant_length=outputs['defendant_length']\n",
    "    label=outputs['label']\n",
    "    decoder_inputs=outputs['decoder_inputs']\n",
    "    loss_weights=outputs['loss_weights']\n",
    "    keep_prob=outputs['keep_prob']\n",
    "    sample_rate=outputs['sample_rate']\n",
    "\n",
    "    #gpu config\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "    gpu_option = tf.GPUOptions(per_process_gpu_memory_fraction = 0.2)\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement = True, gpu_options = gpu_option, log_device_placement = False)\n",
    "\n",
    "    \n",
    "\n",
    "    tvar = tf.trainable_variables()\n",
    "    for v in tvar:\n",
    "        print (v.name)\n",
    "                        \n",
    "                                  \n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    \n",
    "    for path in ckpt.all_model_checkpoint_paths:\n",
    "        \n",
    "        sess = tf.Session(config=session_conf)\n",
    "        saver.restore(sess, path)\n",
    "\n",
    "        batch = 0\n",
    "        test_cost = 0\n",
    "        test_accuracy = 0\n",
    "        test_bleu = 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            datas, is_again = test_utils.next_batch()\n",
    "            if is_again:\n",
    "                break\n",
    "                \n",
    "            batch += 1\n",
    "            \n",
    "            \n",
    "            batch_source = datas['source'] \n",
    "            batch_defendant = datas['defendant'] \n",
    "            batch_defendant_length = datas['defendant_length']\n",
    "            batch_ground_truth = datas['ground_truth']\n",
    "            batch_label = datas['label']\n",
    "            batch_weights = datas['loss_weights']\n",
    "            \n",
    "            feed_dic = {source:batch_source,\n",
    "                        defendant:batch_defendant,\n",
    "                        label:batch_label,\n",
    "                        defendant_length:batch_defendant_length,\n",
    "                        decoder_inputs:batch_ground_truth,\n",
    "                        loss_weights:batch_weights,\n",
    "                        keep_prob:1.,\n",
    "                        sample_rate:1}\n",
    "                \n",
    "            words_, cost_ = sess.run([words_prediction, cost], feed_dict=feed_dic)\n",
    "\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            accu_, bleu_ = test_utils.i2t(words_, to_print = False)\n",
    "            \n",
    "            test_cost += cost_\n",
    "            test_accuracy += accu_\n",
    "            test_bleu += bleu_\n",
    "  \n",
    "\n",
    "            \n",
    "        test_cost /= batch\n",
    "        test_accuracy /= batch\n",
    "        test_bleu /= batch\n",
    "\n",
    "        print (path)\n",
    "        print (\"Loss = \"+\"{:.5f}\".format(test_cost) + \\\n",
    "               \" Accuracy = \"+\"{:.5f}\".format(test_accuracy) + \\\n",
    "               \" BLEU = \" + \"{:.5f}\".format(test_bleu))\n",
    "\n",
    "        sess.close()\n",
    "          \n",
    "        \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    words_path = '/home/xuwenshen/data/big_data/2017_3_13/words'\n",
    "    test_path = '/home/xuwenshen/data/big_data/2017_3_13/test.h5'\n",
    "    \n",
    "    oseq_len = 200\n",
    "    source_len = 1000\n",
    "    simplified_len = 150\n",
    "\n",
    "\n",
    "    batch_size = 50\n",
    "    decoder_hidden = 700\n",
    "    encoder_hidden = 312\n",
    "    source_nfilters = 450\n",
    "    source_width = 3\n",
    "    lstm_layer = 1\n",
    "    embedding_size = 200\n",
    "    \n",
    "    \n",
    "    test_utils = Utils(words_path, test_path, batch_size, nb_samples=10000)\n",
    "\n",
    "    test(test_utils=test_utils,\n",
    "          source_len=source_len,\n",
    "          simplified_len=simplified_len,\n",
    "          oseq_len=oseq_len, \n",
    "          embedding_size=embedding_size,\n",
    "          batch_size=batch_size,\n",
    "          decoder_hidden=decoder_hidden,\n",
    "          encoder_hidden=encoder_hidden,\n",
    "          source_nfilters=source_nfilters,\n",
    "          source_width=source_width,\n",
    "          lstm_layer=lstm_layer,\n",
    "          model_dir='/home/xuwenshen/2017_3_13/attention_cnn_lstm-lstm/model_v1/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

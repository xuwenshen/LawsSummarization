{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3616189d7e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mUtils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_mechanism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/site-packages/hyperboard-0.1.1-py3.4.egg/hyperboard/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/site-packages/hyperboard-0.1.1-py3.4.egg/hyperboard/server.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecorder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflask_httpauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPBasicAuth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjsonify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/site-packages/flask_httpauth.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhashlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSystemRandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwerkzeug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatastructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAuthorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/site-packages/flask/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# utilities we import from Werkzeug and Jinja2 that are unused\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# in the module but are exported as public interface.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwerkzeug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwerkzeug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mredirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/site-packages/werkzeug/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;31m# Due to bootstrapping issues we need to import exceptions here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;31m# Don't ask :-(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'werkzeug.exceptions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/site-packages/werkzeug/exceptions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mimplements_to_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwerkzeug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_exec\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/home/xuwenshen/py3/lib/python3.4/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell, seq2seq\n",
    "from tqdm import *\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "\n",
    "from Utils import Utils\n",
    "import copy_mechanism\n",
    "from hyperboard import Agent\n",
    "\n",
    "    \n",
    "def train(train_utils, valid_utils, source_len, oseq_len, n_echos, simplified_len, decoder_hidden, encoder_hidden,\n",
    "          embedding_size, batch_size, display_step, prob, lstm_layer, learning_rate,\n",
    "          model_dir, source_nfilters, defendant_nfilters, source_width, defendant_width):\n",
    "            \n",
    "    \n",
    "    hyperparameters = {\n",
    "        'learning rate': learning_rate,\n",
    "        'batch size': batch_size,\n",
    "        'criteria': '',\n",
    "        'decoder hidden': decoder_hidden,\n",
    "        'encoder_hidden':encoder_hidden,\n",
    "        'prob': prob,\n",
    "        'source_nfilters': source_nfilters,\n",
    "        'defendant_nfilters': defendant_nfilters, \n",
    "        'source_width': source_width,\n",
    "        'defendant_width': defendant_width, \n",
    "        'lstm_layer': lstm_layer,  \n",
    "        'embedding_size': embedding_size\n",
    "    }\n",
    "        \n",
    "    agent = Agent(port = 5000)\n",
    "    \n",
    "    hyperparameters['criteria'] = 'valid loss'\n",
    "    name_valid_loss = agent.register(hyperparameters, 'cross entropy')\n",
    "    \n",
    "    hyperparameters['criteria'] = 'valid accu'\n",
    "    name_valid_accu = agent.register(hyperparameters, 'accuracy')\n",
    "    \n",
    "    hyperparameters['criteria'] = 'train loss'\n",
    "    name_train_loss = agent.register(hyperparameters, 'cross entropy')\n",
    "    \n",
    "    hyperparameters['criteria'] = 'train accu'\n",
    "    name_train_accu = agent.register(hyperparameters, 'accuracy')\n",
    "    \n",
    "        \n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "        \n",
    "    words_size = train_utils.get_words_size()\n",
    "    \n",
    "    outputs = copy_mechanism.model(words_size=words_size, \n",
    "                                   embedding_size=embedding_size,\n",
    "                                   source_len=source_len,\n",
    "                                   simplified_len=simplified_len,\n",
    "                                   oseq_len=oseq_len, \n",
    "                                   decoder_hidden=decoder_hidden,\n",
    "                                   encoder_hidden=encoder_hidden,\n",
    "                                   source_nfilters=source_nfilters,\n",
    "                                   defendant_nfilters=defendant_nfilters,\n",
    "                                   source_width=source_width,\n",
    "                                   defendant_width=defendant_width,\n",
    "                                   lstm_layer=lstm_layer, \n",
    "                                   batch_size=batch_size, \n",
    "                                   is_train=True)\n",
    "\n",
    "    cost=outputs['cost']\n",
    "    words_prediction=outputs['words_prediction']\n",
    "    source=outputs['source']\n",
    "    defendant=outputs['defendant']\n",
    "    defendant_length=outputs['defendant_length']\n",
    "    label=outputs['label']\n",
    "    decoder_inputs=outputs['decoder_inputs']\n",
    "    loss_weights=outputs['loss_weights']\n",
    "    keep_prob=outputs['keep_prob']\n",
    "    sample_rate=outputs['sample_rate']\n",
    "    \n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='adam_optimizer')\n",
    "    # Compute the gradients for a list of variables.\n",
    "    grads_and_vars = optimizer.compute_gradients(cost)\n",
    "    # grads_and_vars is a list of tuples (gradient, variable).\n",
    "    capped_grads_and_vars = [(tf.clip_by_norm(g, 5), v) for g,v in grads_and_vars]\n",
    "    # Ask the optimizer to apply the capped gradients.\n",
    "    train_op = optimizer.apply_gradients(capped_grads_and_vars)\n",
    "\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "    gpu_option = tf.GPUOptions(per_process_gpu_memory_fraction = 0.99)\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement = True, \n",
    "                                  log_device_placement = False,\n",
    "                                  gpu_options = gpu_option)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    \n",
    "    pre_saver = tf.train.Saver()\n",
    "    ckpt = tf.train.latest_checkpoint('/home/xuwenshen/2017_3_13/model_v2_2/')\n",
    "    pre_saver.restore(sess, ckpt)\n",
    "    \n",
    "    global_steps = 0\n",
    "    \n",
    "    echos = 0\n",
    "    batch = 0\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep = 100)\n",
    "\n",
    "#     sess.run(init)\n",
    "    \n",
    "    tvar = tf.trainable_variables()\n",
    "    for v in tvar:\n",
    "        print (v.name)\n",
    "    \n",
    "    print ('init done')\n",
    "    \n",
    "    train_cost = 0\n",
    "    train_accu = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        datas, is_again = train_utils.next_batch()\n",
    "        \n",
    "        if is_again:\n",
    "            \n",
    "            train_cost = 0\n",
    "            train_accu = 0\n",
    "            echos += 1\n",
    "            batch = 0\n",
    "            if echos == n_echos:\n",
    "                break\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        batch_source = datas['source'] \n",
    "        batch_defendant = datas['defendant'] \n",
    "        batch_defendant_length = datas['defendant_length']\n",
    "        batch_ground_truth = datas['ground_truth']\n",
    "        batch_label = datas['label']\n",
    "        batch_weights = datas['loss_weights']\n",
    "        \n",
    "        batch += 1\n",
    "        \n",
    "\n",
    "        sample_rate_ = min(.5, 0.39 + 0.005 * (global_steps))\n",
    "\n",
    "        \n",
    "        words_, cost_, _= sess.run([words_prediction, cost, train_op], \n",
    "                                   feed_dict={source:batch_source,\n",
    "                                              defendant:batch_defendant,\n",
    "                                              label:batch_label,\n",
    "                                              defendant_length:batch_defendant_length,\n",
    "                                              decoder_inputs:batch_ground_truth,\n",
    "                                              loss_weights:batch_weights,\n",
    "                                              keep_prob:prob,\n",
    "                                              sample_rate:sample_rate_,\n",
    "                                               })\n",
    "    \n",
    "        train_cost += cost_\n",
    "#         train_accu += train_utils.i2t(words_, to_print = False)\n",
    "\n",
    "        \n",
    "        if batch % display_step == 0:\n",
    "            \n",
    "            valid_cost = 0\n",
    "            valid_accu = 0\n",
    "            valid_batchs = 0\n",
    "            \n",
    "            train_cost /= display_step\n",
    "            train_accu /= display_step\n",
    "            \n",
    "            while True:\n",
    "                \n",
    "                datas, is_again = valid_utils.next_batch()\n",
    "                \n",
    "                if is_again:\n",
    "                    break\n",
    "        \n",
    "                batch_source = datas['source'] \n",
    "                batch_defendant = datas['defendant'] \n",
    "                batch_defendant_length = datas['defendant_length']\n",
    "                batch_ground_truth = datas['ground_truth']\n",
    "                batch_label = datas['label']\n",
    "                batch_weights = datas['loss_weights']\n",
    "\n",
    "                valid_batchs += 1\n",
    "                \n",
    "                word_, cost_, = sess.run([words_prediction, cost], \n",
    "                                          feed_dict={source:batch_source,\n",
    "                                                     defendant:batch_defendant,\n",
    "                                                     defendant_length:batch_defendant_length,\n",
    "                                                     label:batch_label,\n",
    "                                                     decoder_inputs:batch_ground_truth,\n",
    "                                                     loss_weights:batch_weights,\n",
    "                                                     keep_prob:1.,\n",
    "                                                     sample_rate:1.,\n",
    "                                                      })\n",
    "                valid_accu += valid_utils.i2t(word_, to_print = True)\n",
    "                valid_cost += cost_\n",
    "            \n",
    "            valid_cost /= valid_batchs\n",
    "            valid_accu /= valid_batchs\n",
    "            \n",
    "            saver.save(sess, model_dir + \\\n",
    "                       \"sample_rate-\" + str(sample_rate_) + \\\n",
    "                       \"-train_accu-{:.4f}\".format(train_accu) + \\\n",
    "                       \"-train_cost-{:.4f}\".format(train_cost) + \\\n",
    "                       \"-valid_accu-{:.4f}\".format(valid_accu) + \\\n",
    "                       \"-valid_cost-{:.4f}\".format(valid_cost) + \\\n",
    "                       \"-model.ckpt\", global_step = global_steps)\n",
    "            \n",
    "            \n",
    "            print (\"Echo: \"+str(echos) + \" Iters: \"+str(batch) + \\\n",
    "                   \" Sample: \" + \"{:.3f}\".format(sample_rate_) + \\\n",
    "                   \" Train loss: \" + \"{:.4f}\".format(train_cost) + \\\n",
    "                   \" Valid loss: \" + \"{:.4f}\".format(valid_cost)+ \\\n",
    "                   \" Train accu: \" + \"{:.4f}\".format(train_accu)+ \\\n",
    "                   \" Valid accu: \" + \"{:.4f}\".format(valid_accu))\n",
    "            \n",
    "\n",
    "            agent.append(name_valid_loss, global_steps, valid_cost)\n",
    "            agent.append(name_valid_accu, global_steps, valid_accu)\n",
    "            agent.append(name_train_loss, global_steps, train_cost)\n",
    "            agent.append(name_train_accu, global_steps, train_accu)\n",
    "            \n",
    "            global_steps += 1\n",
    "            \n",
    "        \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    words_path = '/home/xuwenshen/data/big_data/2017_3_13/words'\n",
    "    train_path = '/home/xuwenshen/data/big_data/2017_3_13/train.h5'\n",
    "    valid_path = '/home/xuwenshen/data/big_data/2017_3_13/valid.h5'\n",
    "    \n",
    "    oseq_len = 200\n",
    "    source_len = 1000\n",
    "    simplified_len = 150\n",
    "\n",
    "    batch_size = 64\n",
    "    display_step = 400\n",
    "    n_echos = 100\n",
    "    prob = 0.75\n",
    "    decoder_hidden = 600\n",
    "    encoder_hidden = 256\n",
    "    source_nfilters = 128\n",
    "    defendant_nfilters = 32\n",
    "    source_width = 3\n",
    "    defendant_width = 3\n",
    "    lstm_layer = 1\n",
    "    learning_rate = 0.001\n",
    "    embedding_size = 200\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    train_utils = Utils(words_path, train_path, batch_size, nb_samples=1600000)\n",
    "    valid_utils = Utils(words_path, valid_path, batch_size, nb_samples=640)\n",
    "\n",
    "    train(train_utils=train_utils,\n",
    "          valid_utils=valid_utils,\n",
    "          source_len=source_len,\n",
    "          simplified_len=simplified_len,\n",
    "          oseq_len=oseq_len,\n",
    "          embedding_size=embedding_size,\n",
    "          batch_size=batch_size,\n",
    "          display_step=display_step,\n",
    "          n_echos=n_echos,\n",
    "          prob=prob,\n",
    "          decoder_hidden=decoder_hidden,\n",
    "          encoder_hidden=encoder_hidden,\n",
    "          source_nfilters=source_nfilters,\n",
    "          defendant_nfilters=defendant_nfilters,\n",
    "          source_width=source_width,\n",
    "          defendant_width=defendant_width,\n",
    "          lstm_layer=lstm_layer,\n",
    "          learning_rate=learning_rate,\n",
    "          model_dir='/home/xuwenshen/2017_3_13/model_v2_2/')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
